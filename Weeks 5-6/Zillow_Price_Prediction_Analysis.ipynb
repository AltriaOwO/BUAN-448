{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963833ac",
   "metadata": {
    "id": "963833ac"
   },
   "source": [
    "# **Zillow Price Prediction Analysis**\n",
    "# PROBLEM\n",
    "It is not necessary to include this in your code file for the final project. However, this is a crucial phase and should be included in your presentation deck. I mention it here to emphasize its relevance and importance in the process.\n",
    "### Problem Statement:\n",
    "Zillow, a major player in real estate, has recently come under scrutiny due to significant fluctuations in housing prices in various regions, particularly in response to external factors like rising interest rates and inflation. These fluctuations have raised concerns about the accuracy of Zillow’s price prediction models, particularly their ability to provide reliable estimates for homes listed on the platform. Zillow’s leadership team has tasked the analytics team with analyzing historical housing data to understand the root causes of pricing inaccuracies and propose improvements to the pricing prediction model.\n",
    "\n",
    "### Key Questions:\n",
    "1. How accurate are Zillow's current price predictions compared to actual selling prices?\n",
    "2. Which factors (e.g., area, number of bedrooms, bathrooms, etc.) have the most significant impact on price prediction errors?\n",
    "3. Are there patterns of over- or under-prediction in specific areas or home types?\n",
    "4. Do any model assumptions (e.g., linearity, multicollinearity) need to be addressed to improve prediction accuracy?\n",
    "5. How can Zillow’s pricing algorithm be improved to adapt to current market volatility?\n",
    "\n",
    "### Root Cause Analysis\n",
    "\n",
    "1. **Why are Zillow’s pricing predictions sometimes inaccurate?**  \n",
    "   Because the model does not account for all market conditions (e.g., sudden interest rate hikes, regional demand fluctuations).\n",
    "\n",
    "2. **Why doesn't the model account for these market conditions?**  \n",
    "   The current variables in the model (e.g., area, number of bedrooms, etc.) are not sufficient to capture dynamic market factors like economic shifts or buyer sentiment.\n",
    "\n",
    "3. **Why weren’t more dynamic factors included in the model?**  \n",
    "   Zillow’s initial model was built based on static historical data, without incorporating real-time data or external economic indicators.\n",
    "\n",
    "4. **Why is the model dependent on static historical data?**  \n",
    "   The original algorithm was designed with a focus on past trends rather than adaptive mechanisms that can adjust based on changing market conditions.\n",
    "\n",
    "5. **Why hasn’t the model been updated to include adaptive features?**  \n",
    "   The decision to update the model has been delayed due to the complexity of incorporating real-time data and the cost of implementing new algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_9QIoNlaQfsL",
   "metadata": {
    "id": "_9QIoNlaQfsL"
   },
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa629fb7",
   "metadata": {
    "id": "aa629fb7"
   },
   "source": [
    "## Import Necessary Libraries\n",
    "### What It Is:\n",
    "This code imports essential libraries and modules needed for data manipulation, visualization, statistical analysis, and machine learning modeling. These libraries provide various tools and functions to perform tasks like data cleaning, plotting, building predictive models, and evaluating model performance.\n",
    "\n",
    "### How It Works:\n",
    "- **Pandas (`pd`)**: Used for data manipulation and analysis. It provides data structures like DataFrames to handle and analyze data efficiently.\n",
    "- **NumPy (`np`)**: Provides support for large, multi-dimensional arrays and matrices, along with mathematical functions to operate on these arrays. It's often used for numerical operations on data.\n",
    "- **Seaborn (`sns`)** and **Matplotlib (`plt`)**: These libraries are used for data visualization. Seaborn provides high-level functions to create attractive and informative statistical graphics, while Matplotlib is a more general-purpose plotting library.\n",
    "- **Scikit-Learn**:\n",
    "  - **`train_test_split`**: Splits the dataset into training and testing sets.\n",
    "  - **`cross_val_score`**: Performs cross-validation to assess model performance.\n",
    "  - **`LinearRegression`, `RidgeCV`, `LassoCV`, `ElasticNetCV`**: These are linear models for regression, including regularized versions (Ridge, Lasso, Elastic Net) that help prevent overfitting.\n",
    "  - **`mean_squared_error`, `r2_score`**: Metrics used to evaluate the performance of regression models.\n",
    "  - **`StandardScaler`**: Standardizes features by removing the mean and scaling to unit variance, which is crucial for models sensitive to the scale of data.\n",
    "- **Statsmodels (`sm`)**: Used for statistical modeling and analysis. It provides functions for building and analyzing statistical models, such as Ordinary Least Squares (OLS) regression.\n",
    "\n",
    "### Why It's Important:\n",
    "Importing these libraries is a foundational step in data science and machine learning projects. They offer a wide range of functionalities needed to handle tasks such as data manipulation, visualization, building models, and evaluating their performance. By using these libraries, you can efficiently work with data, create visual insights, build robust predictive models, and perform statistical analyses, all essential for extracting meaningful information and making data-driven decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc36289",
   "metadata": {
    "id": "dbc36289"
   },
   "outputs": [],
   "source": [
    "# Import libraries for data manipulation, visualization, and modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00d824",
   "metadata": {
    "id": "dc00d824"
   },
   "source": [
    "## Data Loading\n",
    "### What It Is:\n",
    "This code loads a dataset from a CSV file into a pandas DataFrame and provides a quick overview of its structure and contents. This is an initial step in data analysis, allowing you to inspect the data and understand its variables, types, and overall structure.\n",
    "\n",
    "### How It Works:\n",
    "- **Load the Dataset**: `pd.read_csv('Housing.csv')` reads the CSV file named 'Housing.csv' into a pandas DataFrame called `dataset`. This function interprets the data in the file and loads it into a structured format that can be used for further analysis and manipulation.\n",
    "- **Display the First Few Rows**: `display(dataset.head())` shows the first five rows of the dataset. This provides a quick glimpse of the data, including its columns, some sample values, and how the data is structured.\n",
    "- **Check Dataset Info**: `dataset.info()` prints a summary of the dataset, including the number of rows and columns, the data types of each column, and the count of non-null values. This information helps identify which columns contain missing values, the data types of variables, and if any data preprocessing (like type conversion or missing value handling) is needed.\n",
    "\n",
    "### Why It's Important:\n",
    "Loading and inspecting the dataset is a crucial first step in any data analysis or machine learning project. It allows you to understand the data's format, identify potential issues such as missing values or incorrect data types, and get an initial sense of the variables you'll be working with. This initial exploration guides subsequent steps like data cleaning, preprocessing, and modeling, ensuring that you handle the dataset correctly and build robust models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372bb38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "c372bb38",
    "outputId": "97c80bee-712f-4f6b-d3a3-d24d6ea6349e"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv('Housing.csv')\n",
    "\n",
    "# Display first few rows\n",
    "display(dataset.head())\n",
    "\n",
    "# Check dataset info\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0wzJPjNa7wEy",
   "metadata": {
    "id": "0wzJPjNa7wEy"
   },
   "source": [
    "## Data Cleaning\n",
    "### What It Is:\n",
    "This code cleans the dataset by handling missing values and removing duplicates. Cleaning data is an essential step in data preprocessing to ensure that the dataset is ready for analysis and modeling. It improves data quality, reduces biases, and helps prevent errors during the model training process.\n",
    "\n",
    "### How It Works:\n",
    "- **Replace Missing Values with NaN**: `dataset.replace('', np.nan, inplace=True)` replaces any empty strings in the dataset with `NaN` (Not a Number), a standard way to represent missing values in pandas. This makes it easier to identify and handle missing data consistently.\n",
    "- **Impute Missing Numeric Variables**: The code iterates over all numeric columns (`float64` and `int64` types) and fills any `NaN` values with the median of the respective column. Using the median is robust against outliers and ensures that the imputed value does not skew the data distribution as much as the mean might.\n",
    "- **Remove Duplicates**: `dataset.drop_duplicates(inplace=True)` checks for and removes any duplicate rows from the dataset. Duplicates can distort analysis and lead to biased results, so removing them ensures each observation is unique.\n",
    "- **Display Cleaned Data**: `display(dataset.head())` shows the first few rows of the cleaned dataset, allowing you to verify that the data cleaning steps have been applied correctly.\n",
    "\n",
    "### Why It's Important:\n",
    "Handling missing values and removing duplicates are crucial steps in data cleaning. Missing values can lead to biased models or errors during training, and imputing them ensures that the dataset is complete. Using the median for imputation helps maintain the central tendency without being affected by outliers. Removing duplicates prevents redundant data from skewing the analysis, ensuring that the model learns from a clean and accurate dataset. Clean data is essential for building reliable and accurate predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7482c7a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "a7482c7a",
    "outputId": "254d1c43-823c-48c1-d07b-b994f94c6e0c"
   },
   "outputs": [],
   "source": [
    "# Replace missing values with NaN and handle them\n",
    "dataset.replace('', np.nan, inplace=True)\n",
    "\n",
    "# Impute missing numeric variables with the median\n",
    "for col in dataset.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    dataset[col].fillna(dataset[col].median(), inplace=True)\n",
    "\n",
    "# Check for duplicates and remove them\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "\n",
    "# Display the cleaned dataset\n",
    "display(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l7096cIo714t",
   "metadata": {
    "id": "l7096cIo714t"
   },
   "source": [
    "## Handling Categorical Variables\n",
    "### What It Is:\n",
    "This code converts categorical variables in the dataset into numerical form using one-hot encoding. One-hot encoding is a method of representing categorical data as binary columns, making it suitable for inclusion in regression models and other machine learning algorithms that require numerical input.\n",
    "\n",
    "### How It Works:\n",
    "- **One-Hot Encoding**: The `pd.get_dummies` function is used to convert each categorical variable into a series of binary columns (0s and 1s). For example, if a categorical variable has three categories, it will be converted into two binary columns (`drop_first=True` avoids the dummy variable trap by dropping one category to prevent multicollinearity).\n",
    "- **Binary Columns**: For each category in the original categorical variable, a new binary column is created. If a row belongs to a category, the corresponding column gets a value of 1; otherwise, it gets a 0. This allows the machine learning model to understand and use categorical data.\n",
    "- **View Encoded Data**: `dataset_encoded.head()` displays the first few rows of the newly encoded dataset, showing how the categorical variables have been transformed into a numerical format.\n",
    "\n",
    "### Why It's Important:\n",
    "Machine learning models like linear regression require numerical inputs to perform calculations. Categorical variables, in their original form, cannot be directly used in these models. One-hot encoding transforms these categorical variables into a numerical format that the model can understand. This step is crucial for incorporating categorical data into your predictive models, ensuring they can effectively learn from the dataset and make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OwiS82q-3Ij8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "OwiS82q-3Ij8",
    "outputId": "22593b98-43e8-4d84-d846-5780a5d90009"
   },
   "outputs": [],
   "source": [
    "# Convert categorical variables to numeric using one-hot encoding\n",
    "# This will create binary columns for each category in the categorical variables\n",
    "dataset_encoded = pd.get_dummies(dataset, drop_first=True)\n",
    "\n",
    "# Display the first few rows of the encoded dataset\n",
    "dataset_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd900c",
   "metadata": {
    "id": "55fd900c"
   },
   "source": [
    "## Exploratory Data Analysis:\n",
    "### What It Is:\n",
    "This code generates three types of visualizations—box plots, a scatter plot matrix, and a correlation heatmap—to explore and understand the relationships, distributions, and correlations between numeric variables in the dataset. These plots help identify patterns, outliers, and the strength of relationships between variables, providing insights before building predictive models.\n",
    "\n",
    "### How It Works:\n",
    "- **Box Plot**:\n",
    "  - **What It Does**: Creates a box plot for each numeric variable (`area`, `bedrooms`, `bathrooms`, `stories`, `parking`, `price`) to visualize the distribution, spread, and potential outliers.\n",
    "  - **How It Works**: The data is reshaped using `pd.melt` to create a long-format DataFrame, which is passed to `sns.boxplot` for plotting. Each box shows the median, quartiles, and possible outliers for the variable, giving a quick overview of its distribution.\n",
    "- **Scatter Plot Matrix**:\n",
    "  - **What It Does**: Generates a scatter plot matrix to visualize pairwise relationships between the selected variables. This helps identify correlations and potential linear relationships.\n",
    "  - **How It Works**: `sns.pairplot` creates scatter plots for every pair of variables in `scatter_columns`. Diagonal plots typically show the distribution of each variable, while the off-diagonal plots show relationships between pairs of variables, highlighting trends or patterns.\n",
    "- **Correlation Heatmap**:\n",
    "  - **What It Does**: Creates a heatmap of the correlation matrix to show the strength and direction of relationships between numeric variables.\n",
    "  - **How It Works**: `dataset_encoded.corr()` calculates the correlation matrix, measuring how changes in one variable are associated with changes in another. `sns.heatmap` visualizes this matrix, with colors indicating the correlation strength. Annotated values help identify strongly correlated pairs, which is useful in feature selection and multicollinearity detection.\n",
    "\n",
    "### Why It's Important:\n",
    "Visualizations like box plots, scatter plot matrices, and correlation heatmaps are crucial in exploratory data analysis (EDA). They help identify outliers, understand variable distributions, and explore relationships between variables. Box plots can reveal skewness and outliers; scatter plot matrices show pairwise relationships; and correlation heatmaps identify correlated variables, guiding decisions in model building, feature selection, and potential data transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0fc3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "id": "15c0fc3f",
    "outputId": "e5c71197-15d6-404c-957b-ecda9c6dd1d4"
   },
   "outputs": [],
   "source": [
    "# Box plot for numeric variables\n",
    "numeric_columns = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price']\n",
    "dataset_long = pd.melt(dataset, id_vars=['price'], value_vars=numeric_columns)\n",
    "plt.figure(figsize=(12, 8))  # Adjust the width and height as needed\n",
    "sns.boxplot(x='variable', y='value', data=dataset_long)\n",
    "plt.title('Box Plot of Numeric Variables')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5g7WbSBNnmtq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5g7WbSBNnmtq",
    "outputId": "1e27b39e-1b85-414e-eac6-3bf7ef96c1a8"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter Plot Matrix\n",
    "# Select the columns for the scatter plot matrix\n",
    "scatter_columns = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price']\n",
    "\n",
    "# Create the scatter plot matrix\n",
    "sns.pairplot(dataset[scatter_columns])\n",
    "plt.suptitle('Scatter Plot Matrix of Selected Variables', y=1.02)  # Adjust the title position\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c616343",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "9c616343",
    "outputId": "c0d25554-8422-4630-cd38-dcb7f3583f3f"
   },
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "corr_matrix = dataset_encoded.corr()\n",
    "plt.figure(figsize=(12, 8))  # Adjust the width and height as needed\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nztxAbiHQYTG",
   "metadata": {
    "id": "nztxAbiHQYTG"
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f9290",
   "metadata": {
    "id": "ad9f9290"
   },
   "source": [
    "## Training Model\n",
    "### What It Is:\n",
    "This code preprocesses the dataset, splits it into training and testing sets, scales the features, and fits a Multiple Linear Regression model. This process is essential for building and evaluating a linear regression model that predicts a target variable based on several predictors.\n",
    "\n",
    "### How It Works:\n",
    "- **Convert Categorical Variables**: `pd.get_dummies(dataset, drop_first=True)` converts categorical variables into numerical form using one-hot encoding, creating binary columns for each category. This step is crucial because regression models require numerical input.\n",
    "- **Split the Dataset**: The code splits the dataset into features (`X`) and the target variable (`y`). It then divides the data into training and testing sets, with 80% used for training and 20% for testing, ensuring the model is trained and evaluated on separate data.\n",
    "- **Feature Scaling**: `StandardScaler` standardizes the features by removing the mean and scaling to unit variance. This step is important for linear regression, especially when the features have different scales, ensuring that each feature contributes equally to the model.\n",
    "- **Fit the Multiple Linear Regression Model**: `LinearRegression().fit()` trains the model on the scaled training data, finding the best-fitting line that minimizes the sum of squared differences between the observed and predicted values.\n",
    "\n",
    "### Why It's Important:\n",
    "Preprocessing steps like one-hot encoding and feature scaling are vital to prepare the data for linear regression. Splitting the data into training and testing sets allows for an unbiased evaluation of the model's performance. By fitting the Multiple Linear Regression model, we can identify relationships between the predictors and the target variable, enabling predictions and insights into the factors influencing the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b5589",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "680b5589",
    "outputId": "e4942bb3-ffd0-4dbd-87f9-cbe0a5792364"
   },
   "outputs": [],
   "source": [
    "# Convert categorical variables to numerical using one-hot encoding\n",
    "dataset = pd.get_dummies(dataset, drop_first=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = dataset.drop('price', axis=1)\n",
    "y = dataset['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the Multiple Linear Regression Model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FB4sbgN6-Yei",
   "metadata": {
    "id": "FB4sbgN6-Yei"
   },
   "source": [
    "## Training Model Output with OLS\n",
    "### What It Is:\n",
    "This code fits an Ordinary Least Squares (OLS) regression model using the `statsmodels` library and provides a detailed statistical summary of the model. OLS is a method for estimating the parameters in a linear regression model, giving insights into the relationship between the predictors and the target variable.\n",
    "\n",
    "### How It Works:\n",
    "- **Add a Constant (Intercept)**: The `sm.add_constant(X_train_scaled)` function adds an intercept term to the model, which represents the base value of the target variable when all predictors are zero. Including the intercept is essential for accurately modeling the relationship between the features and the target.\n",
    "- **Fit the Model**: The code uses `sm.OLS()` to fit the OLS regression model to the training data (`X_train_const` and `y_train`). This method calculates the coefficients that minimize the sum of squared differences between observed and predicted values.\n",
    "- **Print Model Summary**: `sm_model.summary()` outputs a comprehensive summary, including coefficients, standard errors, t-values, p-values, R-squared, and more. This summary provides detailed insights into how each predictor influences the target variable and how well the model fits the data.\n",
    "\n",
    "### Why It's Important:\n",
    "The OLS model summary gives a deeper understanding of the linear regression model. It shows which predictors significantly impact the target variable (based on p-values), how well the model fits the data (R-squared and Adjusted R-squared), and the statistical significance of the overall model. This detailed analysis helps in interpreting the model's results, understanding the underlying relationships, and making informed decisions about the model's validity and effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_lmtQ0wY-VOm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_lmtQ0wY-VOm",
    "outputId": "9e346abb-f3a0-46e5-80ba-24e56d6504fc"
   },
   "outputs": [],
   "source": [
    "# Convert boolean columns to integers (0, 1)\n",
    "#print(dataset)\n",
    "dataset = dataset.astype(int)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = dataset.drop('price', axis=1)\n",
    "y = dataset['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Add a constant (intercept) to the features\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "\n",
    "# Fit the OLS model\n",
    "sm_model = sm.OLS(y_train, X_train_const).fit()\n",
    "\n",
    "# Print the detailed model summary\n",
    "print(sm_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e977db",
   "metadata": {
    "id": "98e977db"
   },
   "source": [
    "## Modeling with Testing Data\n",
    "\n",
    "### What It Is:\n",
    "This code evaluates the performance of a linear regression model by making predictions on the test dataset and calculating key metrics, including Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared, and Adjusted R-squared. These metrics help assess how well the model predicts the target variable.\n",
    "\n",
    "### How It Works:\n",
    "- **Make Predictions**: The model makes predictions on the test data (`X_test_scaled`).\n",
    "- **Calculate MSE and RMSE**:\n",
    "  - **MSE** measures the average squared difference between actual and predicted values, indicating how well the model's predictions match the real data.\n",
    "  - **RMSE** is the square root of MSE, providing an error metric in the same units as the target variable.\n",
    "- **Calculate R-squared**: This metric shows the proportion of variance in the target variable explained by the model. An R-squared closer to 1 indicates a better fit.\n",
    "- **Calculate Adjusted R-squared**: Adjusted R-squared accounts for the number of predictors in the model, providing a more accurate measure when comparing models with different numbers of features. It penalizes the model for including unnecessary predictors that do not improve model performance.\n",
    "\n",
    "### Why It's Important:\n",
    "These evaluation metrics provide a comprehensive understanding of the model's performance. RMSE gives insight into the average prediction error, while R-squared and Adjusted R-squared indicate how well the model explains the variance in the target variable. Adjusted R-squared is particularly important when using multiple predictors, as it adjusts for the number of features, helping to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc812881",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc812881",
    "outputId": "720ab1ab-958a-451f-9e10-858d621fb1d8"
   },
   "outputs": [],
   "source": [
    "# Make predictions and evaluate\n",
    "predictions = linear_model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "r_squared = r2_score(y_test, predictions)\n",
    "\n",
    "# Calculate Adjusted R-squared\n",
    "n = X_test.shape[0]  # number of observations\n",
    "p = X_test.shape[1]  # number of predictors\n",
    "adjusted_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "\n",
    "print(\"\\nModel Performance on Test Set:\")\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "print(f'R-squared: {r_squared}')\n",
    "print(f'Adjusted R-squared: {adjusted_r_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZAbaZHlZPsVt",
   "metadata": {
    "id": "ZAbaZHlZPsVt"
   },
   "source": [
    "# ADVANCED CONCEPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0059bdf2",
   "metadata": {
    "id": "0059bdf2"
   },
   "source": [
    "## Regularization with Ridge, Lasso, and Elastic Net\n",
    "\n",
    "### What It Is:\n",
    "This code implements three regularized linear regression models: **Ridge Regression**, **Lasso Regression**, and **Elastic Net Regression**. These models add penalties to the model's coefficients to handle multicollinearity, reduce complexity, and prevent overfitting. Regularization improves model performance, especially when dealing with many features or correlated variables.\n",
    "\n",
    "### How It Works:\n",
    "- **Ridge Regression**: Adds an L2 penalty (squared coefficients) to shrink coefficients toward zero. The code uses `RidgeCV` to find the best regularization strength (`alpha`) using cross-validation. It reduces variance in models with correlated features without removing any features.\n",
    "- **Lasso Regression**: Adds an L1 penalty (absolute coefficients), which can shrink some coefficients to exactly zero, effectively performing feature selection. `LassoCV` finds the best `alpha` using cross-validation. It is useful for identifying the most important features by eliminating irrelevant ones.\n",
    "- **Elastic Net Regression**: Combines Lasso and Ridge by using both L1 and L2 penalties. `ElasticNetCV` finds the best `alpha` and `l1_ratio` to balance between the two penalties. This method is helpful when dealing with multiple correlated features, offering both shrinkage and feature selection.\n",
    "\n",
    "### Why It's Important:\n",
    "Regularization methods like Ridge, Lasso, and Elastic Net improve model performance by reducing overfitting and handling multicollinearity. They help create simpler, more interpretable models, especially in datasets with many features. By comparing RMSE and R-squared values, you can select the best regularization method for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca4c25c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bca4c25c",
    "outputId": "153f45b9-3dcd-443d-aa34-e157eb57bbaf"
   },
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "ridge_model = RidgeCV(alphas=np.logspace(-4, 4, 100), cv=10)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "ridge_predictions = ridge_model.predict(X_test_scaled)\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_predictions))\n",
    "ridge_r2 = r2_score(y_test, ridge_predictions)\n",
    "\n",
    "# Output the results\n",
    "print(f'Ridge RMSE: {ridge_rmse}')\n",
    "print(f'Ridge R-squared: {ridge_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d481df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74d481df",
    "outputId": "b82b2b28-0888-40bc-d1d2-e54d55511a48"
   },
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "lasso_model = LassoCV(alphas=np.logspace(-4, 4, 100), cv=10)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "lasso_predictions = lasso_model.predict(X_test_scaled)\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_predictions))\n",
    "lasso_r2 = r2_score(y_test, lasso_predictions)\n",
    "\n",
    "print(f'Lasso RMSE: {lasso_rmse}')\n",
    "print(f'Lasso R-squared: {lasso_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a480a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "322a480a",
    "outputId": "86521eb5-52ba-48a0-f689-df30ab0b8e18"
   },
   "outputs": [],
   "source": [
    "# Elastic Net Regression\n",
    "elasticnet_model = ElasticNetCV(alphas=np.logspace(-4, 4, 100), l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=10)\n",
    "elasticnet_model.fit(X_train_scaled, y_train)\n",
    "elasticnet_predictions = elasticnet_model.predict(X_test_scaled)\n",
    "elasticnet_rmse = np.sqrt(mean_squared_error(y_test, elasticnet_predictions))\n",
    "elasticnet_r2 = r2_score(y_test, elasticnet_predictions)\n",
    "\n",
    "print(f'Elastic Net RMSE: {elasticnet_rmse}')\n",
    "print(f'Elastic Net R-squared: {elasticnet_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f941306d",
   "metadata": {
    "id": "f941306d"
   },
   "source": [
    "## Outlier Removal with IQR\n",
    "### What It Is:\n",
    "This code defines a function called `remove_outliers` that uses the Interquartile Range (IQR) method to identify and remove outliers from a dataset. Outliers are data points that significantly deviate from the rest of the dataset and can skew the results of data analysis, especially in linear regression. By removing these extreme values, you can improve the accuracy and robustness of your model.\n",
    "\n",
    "### How It Works:\n",
    "1. **IQR Calculation**: For each specified column, the function calculates the 25th percentile (Q1) and the 75th percentile (Q3). The IQR is the difference between Q3 and Q1 and represents the middle 50% of the data.\n",
    "2. **Define Boundaries**: It defines the lower and upper boundaries for outliers:\n",
    "   - **Lower Bound**: \\( Q1 - 1.5 * IQR \\)\n",
    "   - **Upper Bound**: \\( Q3 + 1.5 * IQR \\)\n",
    "   - These boundaries help identify data points that lie significantly outside the interquartile range.\n",
    "3. **Filter Data**: The function filters the dataset to include only data points that fall within these boundaries for each specified column. Data points outside these bounds are considered outliers and are removed.\n",
    "4. **Return Cleaned Data**: The function returns a new DataFrame (`dataset_clean`) with outliers removed for the specified columns.\n",
    "\n",
    "### Why It's Important:\n",
    "Outliers can have a disproportionate effect on statistical analyses and models. In linear regression, outliers can skew the results, leading to misleading coefficients, inflated error metrics, and poor predictive performance. By using the IQR method to remove outliers, you ensure that the analysis focuses on the main body of the data, improving the model's reliability and interpretability. This method is robust and simple, providing a quick way to clean data and enhance the quality of your statistical models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2a80c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "fef2a80c",
    "outputId": "d6745af6-76a4-453d-8808-089a79313d7d"
   },
   "outputs": [],
   "source": [
    "# Define function to remove outliers using IQR\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "# Remove outliers\n",
    "dataset_clean = remove_outliers(dataset, numeric_columns)\n",
    "display(dataset_clean.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa23cea",
   "metadata": {
    "id": "cfa23cea"
   },
   "source": [
    "## Assumptions of Multiple Linear Regression\n",
    "### What It Is:\n",
    "This code visually and statistically checks the assumptions of multiple linear regression (MLR). In linear regression, several assumptions must be satisfied for the model to be valid and reliable. This code helps verify these assumptions, including linearity, homoscedasticity, normality of residuals, independence, and multicollinearity.\n",
    "\n",
    "### How It Works:\n",
    "1. **Linearity**: A scatter plot of actual versus predicted values is created to assess the linearity assumption. Ideally, the points should be close to the red identity line, indicating a linear relationship between the features and the target variable.\n",
    "2. **Homoscedasticity**: The residuals (errors) are plotted against the predicted values. This plot helps check if the variance of residuals is constant across all levels of predicted values. If the residuals scatter randomly around the horizontal line (zero), it indicates homoscedasticity.\n",
    "3. **Normality of Residuals**: A Q-Q plot is generated to check if the residuals follow a normal distribution. In this plot, residuals should roughly follow the 45-degree line, suggesting that they are normally distributed.\n",
    "4. **Independence**: The Durbin-Watson test statistic is calculated to check for autocorrelation in residuals. Values close to 2 suggest that the residuals are independent, which is an essential assumption in linear regression.\n",
    "5. **Multicollinearity**: The Variance Inflation Factor (VIF) is calculated for each feature to detect multicollinearity. VIF values greater than 5 or 10 indicate that the feature has a high correlation with other features, which can distort the model's coefficients.\n",
    "\n",
    "### Why It's Important:\n",
    "Verifying these assumptions is crucial to ensure the reliability of a multiple linear regression model. If the assumptions are violated, the model's estimates and predictions may not be trustworthy. For example, if the residuals are not homoscedastic or normally distributed, it suggests that the model may not be capturing the relationship between variables correctly. Multicollinearity can inflate the variance of the coefficient estimates, making it hard to determine the individual effect of each predictor. By checking these assumptions, you can diagnose potential issues with the model and make necessary adjustments, ensuring that the linear regression results are both valid and interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PQxC6bLmjHKj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PQxC6bLmjHKj",
    "outputId": "1be897af-f58a-4c72-80a7-da58ac8cb739"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Data Visualization for MLR Assumptions\n",
    "\n",
    "# Linearity (scatter plot of actual vs predicted values)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, predictions, color='blue', alpha=0.6)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)  # Identity line\n",
    "plt.title('Actual vs Predicted Prices')\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.show()\n",
    "\n",
    "# Homoscedasticity (residual plot)\n",
    "residuals = y_test - predictions\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(predictions, residuals, alpha=0.6)\n",
    "plt.axhline(0, color='red', linewidth=2)\n",
    "sns.regplot(x=predictions, y=residuals, scatter=False, lowess=True, color='red')\n",
    "plt.title('Residuals vs Fitted Values')\n",
    "plt.xlabel('Fitted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "# Normality of Residuals (Q-Q plot)\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "\n",
    "normalized_residuals = zscore(residuals)\n",
    "sm.qqplot(normalized_residuals, line='45')\n",
    "plt.title('Q-Q Plot of Normalized Residuals')\n",
    "plt.show()\n",
    "# print(\"Residuals:\", residuals[:10])  # Print the first 10 residuals to inspect\n",
    "\n",
    "# Independence\n",
    "# Perform the Durbin-Watson test\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "dw_test = durbin_watson(residuals)\n",
    "print(f\"Durbin-Watson test statistic: {dw_test}\")\n",
    "\n",
    "# Multicollinearity (VIF)\n",
    "# Adding a constant for VIF calculation\n",
    "X_train_const = sm.add_constant(X_train_scaled)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"Feature\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X_train_const, i) for i in range(1, X_train_const.shape[1])]  # Starting at 1 to skip the constant\n",
    "print(vif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fVM_83A1A_RB",
   "metadata": {
    "id": "fVM_83A1A_RB"
   },
   "source": [
    "## Cross-Validation\n",
    "### What It Is:\n",
    "This code uses 10-fold cross-validation to evaluate the performance of a multiple linear regression model. Cross-validation is a technique used to assess how well a model generalizes to new, unseen data by splitting the dataset into multiple subsets (folds) and performing repeated training and evaluation. This approach provides a more robust estimate of the model's performance than a single train-test split.\n",
    "\n",
    "### How It Works:\n",
    "- **Set Up Cross-Validation**: Initializes a linear regression model and sets up 10-fold cross-validation. The data is split into 10 subsets, and the model is trained 10 times, each time using a different subset for validation.\n",
    "- **Cross-Validation Metrics**:\n",
    "  - **RMSE (Root Mean Squared Error)**: `cross_val_score` calculates the negative mean squared error for each fold. Taking the square root and averaging gives the mean RMSE across all folds, indicating the average prediction error.\n",
    "  - **R-squared**: `cross_val_score` also calculates R-squared for each fold, which measures how well the model explains the variance in the target variable. The mean R-squared across folds provides an overall measure of model performance.\n",
    "- **Predictions and Summary**:\n",
    "  - **Cross-Validated Predictions**: `cross_val_predict` generates predictions for each data point using cross-validation, ensuring each prediction is made by a model that didn't see that data point during training.\n",
    "  - **Evaluate Predictions**: Calculates the RMSE and R-squared for the cross-validated predictions on the entire training set, offering a summary of the model's performance.\n",
    "\n",
    "### Why It's Important:\n",
    "Cross-validation is essential for evaluating the performance and generalizability of a model. It helps detect overfitting by assessing how the model performs on different subsets of the data. Using metrics like RMSE and R-squared across multiple folds provides a more reliable estimate of model accuracy, ensuring that the model is not just fitting the training data but can also perform well on new, unseen data. This makes cross-validation a crucial step in building robust and reliable predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1zeDEGAsk1Tl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1zeDEGAsk1Tl",
    "outputId": "79ce2473-e5b6-4d39-da8b-f2a99376a8f0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Set up 10-fold cross-validation\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the Multiple Linear Regression Model with cross-validation\n",
    "# Performing cross-validation and calculating RMSE for each fold\n",
    "cv_rmse = np.sqrt(-cross_val_score(linear_model, X_train_scaled, y_train, cv=10, scoring='neg_mean_squared_error'))\n",
    "cv_r2 = cross_val_score(linear_model, X_train_scaled, y_train, cv=10, scoring='r2')\n",
    "\n",
    "# Calculate the mean RMSE and R-squared across all folds\n",
    "mean_cv_rmse = np.mean(cv_rmse)\n",
    "mean_cv_r2 = np.mean(cv_r2)\n",
    "\n",
    "print(f\"Mean Fold Cross-validated RMSE: {mean_cv_rmse}\")\n",
    "print(f\"Mean Fold Cross-validated R-squared: {mean_cv_r2}\")\n",
    "\n",
    "# View model summary using cross-validation predictions\n",
    "# Getting predictions using cross-validation\n",
    "cv_predictions = cross_val_predict(linear_model, X_train_scaled, y_train, cv=10)\n",
    "\n",
    "# Evaluating the cross-validated model performance\n",
    "cv_rmse_summary = np.sqrt(mean_squared_error(y_train, cv_predictions))\n",
    "cv_r2_summary = r2_score(y_train, cv_predictions)\n",
    "\n",
    "print(f\"Cross-validated RMSE (Summary): {cv_rmse_summary}\")\n",
    "print(f\"Cross-validated R-squared (Summary): {cv_r2_summary}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e74f526",
   "metadata": {
    "id": "5e74f526"
   },
   "source": [
    "# IMPACT\n",
    "It is not necessary to include this in your code file for the final project. However, this is a crucial phase and should be included in your presentation deck. I mention it here to emphasize its relevance and importance in the process.\n",
    "\n",
    "Go back to the mini case and focus on answering the questions. Share your findings and provide a managerial recommendation. Two sentences should suffice for each. \"We find...\" and \"Therefore, we suggest...\" Try to connect some part of the managerial recommendation to your 5 Whys drivers. Once complete, we will debrief this mini case as a group."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
