{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5066efc1-782b-40eb-be9e-3e24e6bb9cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michaelrivera/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelrivera/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/michaelrivera/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 194ms/step\n",
      "WARNING:tensorflow:5 out of the last 41 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x338a33790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "The updated data file with advanced NLP features has been saved as 'IMDB Dataset_with_new_NLP_features.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Bidirectional, Dropout\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "import spacy.cli\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure stopwords and other NLTK resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Check and download the spaCy model if not already installed\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Model 'en_core_web_sm' not found. Downloading it now...\")\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize VADER Sentiment Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Predefined stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load the data\n",
    "def load_data(file_name):\n",
    "    return pd.read_csv(file_name)\n",
    "\n",
    "# Tokenizer for Neural Network Methods\n",
    "def tokenize_and_pad(texts, max_len=128, vocab_size=10000):\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "    return padded_sequences, tokenizer\n",
    "\n",
    "# Extract NLP features\n",
    "def generate_advanced_nlp_features(df):\n",
    "    # Feature 1: Lexical Diversity\n",
    "    df['lexical_diversity'] = df['text'].apply(lambda x: len(set(x.split())) / len(x.split()))\n",
    "\n",
    "    # Feature 2: Readability Score (Flesch Reading Ease)\n",
    "    def flesch_reading_ease(text):\n",
    "        total_words = len(text.split())\n",
    "        total_sentences = len(TextBlob(text).sentences)\n",
    "        syllables = sum(len(word) for word in text.split())  # Simplified syllable approximation\n",
    "        if total_sentences == 0 or total_words == 0:\n",
    "            return 0\n",
    "        return 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (syllables / total_words)\n",
    "    \n",
    "    df['readability_score'] = df['text'].apply(flesch_reading_ease)\n",
    "\n",
    "    # Feature 3: Average Sentence Length\n",
    "    df['avg_sentence_length'] = df['text'].apply(lambda x: np.mean([len(sentence.split()) for sentence in TextBlob(x).sentences]))\n",
    "\n",
    "    # Feature 4 & 5: Sentiment Polarity & Subjectivity\n",
    "    df['sentiment_score'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df['sentiment_subjectivity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "    # Feature 6: Presence of Negation Words\n",
    "    negation_words = {\"not\", \"no\", \"never\", \"none\"}\n",
    "    df['negation_count'] = df['text'].apply(lambda x: sum(word in negation_words for word in x.lower().split()))\n",
    "\n",
    "    # Feature 7: TF-IDF Weighted Scores for Key Phrases\n",
    "    vectorizer = TfidfVectorizer(max_features=5)\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "    df['tfidf_top_keyword_sum'] = tfidf_matrix.sum(axis=1).A1\n",
    "\n",
    "    # Feature 8: Count of Action Words (Verbs)\n",
    "    df['action_word_count'] = df['text'].apply(lambda x: len([word for word, pos in nltk.pos_tag(x.split()) if pos.startswith('VB')]))\n",
    "\n",
    "    # Feature 9: Named Entity Recognition (NER) presence\n",
    "    features = {\"movie\", \"actor\", \"entertainment\", \"studio\", \"award\"}\n",
    "    df['feature_mentions'] = df['text'].apply(lambda x: sum(word in features for word in x.lower().split()))\n",
    "\n",
    "    # Feature 10: Word Frequency Ratio for Top Adjectives\n",
    "    df['adjective_ratio'] = df['text'].apply(lambda x: len([word for word, pos in nltk.pos_tag(x.split()) if pos.startswith('JJ')]) / len(x.split()))\n",
    "\n",
    "    # Feature 11: Topic Modeling via LDA (Similarity Score)\n",
    "    lda = LDA(n_components=3, random_state=42)\n",
    "    lda_features = lda.fit_transform(tfidf_matrix)\n",
    "    df['lda_topic_similarity'] = np.max(lda_features, axis=1)\n",
    "\n",
    "    # Feature 12: Noun-Verb Ratio\n",
    "    df['noun_verb_ratio'] = df['text'].apply(lambda x: len([word for word, pos in nltk.pos_tag(x.split()) if pos.startswith('NN')]) / (1 + len([word for word, pos in nltk.pos_tag(x.split()) if pos.startswith('VB')])))\n",
    "\n",
    "    # Feature 13: Cosine Similarity to Template\n",
    "    template = \"This is a good movie with lots of entertainment value.\"\n",
    "    vector_template = vectorizer.transform([template])\n",
    "    df['cosine_similarity_to_template'] = (tfidf_matrix @ vector_template.T).toarray().flatten()\n",
    "\n",
    "    # Feature 14: Syntactic Complexity (Compound vs Simple sentence ratio)\n",
    "    def syntactic_complexity(text):\n",
    "        sentences = TextBlob(text).sentences\n",
    "        simple_count = len([s for s in sentences if len(s.split()) <= 10])\n",
    "        compound_count = len([s for s in sentences if len(s.split()) > 10])\n",
    "        return compound_count / (1 + simple_count)\n",
    "    \n",
    "    df['syntactic_complexity'] = df['text'].apply(syntactic_complexity)\n",
    "\n",
    "    # Feature 15: Sentiment Scores with VADER\n",
    "    df['vader_sentiment'] = df['text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "\n",
    "    # Feature 16: Named Entity Counts with spaCy\n",
    "    def entity_counts(text):\n",
    "        doc = nlp(text)\n",
    "        entities = {'PERSON': 0, 'ORG': 0, 'GPE': 0}\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in entities:\n",
    "                entities[ent.label_] += 1\n",
    "        return entities\n",
    "\n",
    "    df['person_count'] = df['text'].apply(lambda x: entity_counts(x)['PERSON'])\n",
    "    df['organization_count'] = df['text'].apply(lambda x: entity_counts(x)['ORG'])\n",
    "    df['location_count'] = df['text'].apply(lambda x: entity_counts(x)['GPE'])\n",
    "\n",
    "    # Feature 17: Bi-Gram Frequency\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=5)\n",
    "    bigram_matrix = bigram_vectorizer.fit_transform(df['text'])\n",
    "    df['bigram_top_keyword_sum'] = bigram_matrix.sum(axis=1).A1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate Neural Network-based NLP features\n",
    "def generate_nn_nlp_features(df):\n",
    "    texts = df['text'].values\n",
    "\n",
    "    # Feature 18: Word Embeddings (using pre-trained GloVe embeddings)\n",
    "    max_len = 100\n",
    "    vocab_size = 10000\n",
    "    embedding_dim = 50\n",
    "    padded_sequences, tokenizer = tokenize_and_pad(texts, max_len, vocab_size)\n",
    "\n",
    "    # Create embedding layer using GloVe pre-trained vectors\n",
    "    embeddings_index = {}\n",
    "    try:\n",
    "        with open('glove.6B.50d.txt', 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "    except FileNotFoundError:\n",
    "        print(\"GloVe embeddings file not found. Please ensure 'glove.6B.50d.txt' is available.\")\n",
    "        return df\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index < vocab_size:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "\n",
    "    # Feature 19: Sentence Embeddings via Bidirectional LSTM\n",
    "    model_lstm = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), trainable=False),\n",
    "        Bidirectional(LSTM(64)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model_lstm.compile(optimizer='adam', loss='mse')\n",
    "    lstm_features = model_lstm.predict(padded_sequences)\n",
    "\n",
    "    # Feature 20: Sentence Embeddings via BERT Pipeline with Truncation\n",
    "    bert_embedder = pipeline(\"feature-extraction\", model=\"bert-base-uncased\", tokenizer=\"bert-base-uncased\", device=0 if tf.test.is_gpu_available() else -1)\n",
    "\n",
    "    def bert_embeddings(texts):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            inputs = bert_embedder(text, truncation=True, max_length=512, padding='max_length')\n",
    "            embeddings.append(np.mean(inputs[0], axis=0))\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    bert_embeds = bert_embeddings(texts)\n",
    "\n",
    "    # Feature 21: Cosine Similarity between BERT Embeddings and Template Embedding\n",
    "    template = \"This is a comfortable, modern house with plenty of amenities and spacious rooms.\"\n",
    "    template_embed = bert_embeddings([template])[0]\n",
    "    cosine_similarities = [cosine_similarity([bert_embeds[i]], [template_embed])[0][0] for i in range(len(bert_embeds))]\n",
    "\n",
    "    # Feature 22: RNN-Based Sentence Complexity (using GRU)\n",
    "    model_gru = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), trainable=False),\n",
    "        GRU(64),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model_gru.compile(optimizer='adam', loss='mse')\n",
    "    complexity_scores = model_gru.predict(padded_sequences)\n",
    "\n",
    "    # Add features to DataFrame\n",
    "    df['lstm_features'] = lstm_features.flatten()\n",
    "    df['bert_embedding_mean'] = bert_embeds.mean(axis=1)\n",
    "    df['cosine_similarity_template'] = cosine_similarities\n",
    "    df['rnn_complexity_score'] = complexity_scores.flatten()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Main function to load data, process it, and save the output\n",
    "def main(input_file):\n",
    "    df = load_data(input_file)\n",
    "    processed_data = generate_advanced_nlp_features(df)\n",
    "    processed_data = generate_nn_nlp_features(processed_data)\n",
    "\n",
    "    # Create output file name\n",
    "    base_name = os.path.splitext(input_file)[0]\n",
    "    output_file = f\"{base_name}_with_new_NLP_features.csv\"\n",
    "\n",
    "    # Save processed data\n",
    "    processed_data.to_csv(output_file, index=False)\n",
    "    print(f\"The updated data file with advanced NLP features has been saved as '{output_file}'.\")\n",
    "\n",
    "# Example usage:\n",
    "main(\"IMDB Dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f26863-2ec2-4b51-82af-47f6a4409de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
