{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321baf62",
   "metadata": {
    "id": "321baf62"
   },
   "source": [
    "# Problem\n",
    "In our mini case brief, we highlight the problem, root cause analysis, and key research questions.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38074d",
   "metadata": {
    "id": "7e38074d"
   },
   "source": [
    "# Data\n",
    "This script is for data preprocessing and initial analysis in a customer churn prediction project. It loads necessary libraries, prepares the dataset, and performs basic data cleaning steps.\n",
    "\n",
    "Libraries:\n",
    "* pandas: For data manipulation and handling missing values.\n",
    "* numpy: For numerical operations.\n",
    "* scikit-learn: For partitioning the dataset, training models, and evaluation.\n",
    "* matplotlib: For data visualization.\n",
    "\n",
    "Step 1: Load the Dataset\n",
    "* Reads the customer churn dataset from a CSV file.\n",
    "* The dataset should include customer information relevant for churn prediction.\n",
    "\n",
    "Step 2: Data Cleaning and Preprocessing\n",
    "* Converts the 'Churn' column to a binary outcome (1 for 'Yes', 0 for 'No') for classification.\n",
    "* Removes unnecessary columns (e.g., 'customerID') to focus on relevant features.\n",
    "* Imputes missing values in numeric columns using the median value. This is crucial to ensure the dataset is complete for modeling.\n",
    "* Converts categorical variables to dummy variables for better handling in predictive models.\n",
    "\n",
    "Importance:\n",
    "* This preprocessing is vital to prepare the data for machine learning models.\n",
    "* Clean and well-structured data improves model accuracy and performance.\n",
    "* Proper handling of missing values and categorical variables ensures that the model training process is robust and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ae89d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "859ae89d",
    "outputId": "39d478eb-b305-4ad3-dbcf-07fbd308e230"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "# Replace 'Telco-Customer-Churn.csv' with the path to your dataset\n",
    "dataset_path = 'Telco-Customer-Churn.csv'\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the first few rows of the dataset for inspection\n",
    "print(dataset.head())\n",
    "\n",
    "# Step 2: Data Cleaning and Preprocessing\n",
    "# Convert 'Churn' column to a binary outcome (1 for 'Yes', 0 for 'No')\n",
    "label_encoder = LabelEncoder()\n",
    "dataset['Churn'] = label_encoder.fit_transform(dataset['Churn'])  # 'Yes' -> 1, 'No' -> 0\n",
    "\n",
    "# Remove unnecessary columns (e.g., customerID)\n",
    "if 'customerID' in dataset.columns:\n",
    "    dataset = dataset.drop(columns=['customerID'])\n",
    "\n",
    "dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Impute missing values in numeric columns using the median\n",
    "for col in dataset.select_dtypes(include=[np.number]).columns:\n",
    "    dataset[col] = dataset[col].fillna(dataset[col].median())\n",
    "\n",
    "# Convert categorical variables to dummy variables\n",
    "dataset = pd.get_dummies(dataset, drop_first=True)\n",
    "\n",
    "# Display the cleaned dataset for verification\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d5fc96",
   "metadata": {
    "id": "66d5fc96"
   },
   "source": [
    "# Model\n",
    "This section of the script performs data splitting, model training, prediction, and evaluation for a logistic regression model aimed at predicting customer churn.\n",
    "    \n",
    "* Step 3: Split the Dataset into Training and Testing Sets\n",
    "  * The dataset is split into training (80%) and testing (20%) sets using scikit-learn.\n",
    "  * A random state is set for reproducibility, ensuring consistent results across runs.\n",
    "  * The training set is used to train the model, while the testing set is used to evaluate it.\n",
    "  \n",
    "* Step 4: Fit the Logistic Regression Model\n",
    "  * A logistic regression model is trained using the training data.\n",
    "  * The model predicts the probability of customer churn based on the features in the dataset.\n",
    "  * This step is crucial to identify the relationship between predictors and the outcome (Churn).\n",
    "  \n",
    "* Step 5: Model Summary\n",
    "  * Since scikit-learn's LogisticRegression does not provide a summary directly, we will print the coefficients and intercept of the model to understand the relationship between variables and churn.\n",
    "  \n",
    "* Step 6: Make Predictions on the Test Data\n",
    "  * The model is used to make predictions on the test data.\n",
    "  * Probabilities of churn are converted to binary predictions (1 for 'Yes', 0 for 'No') using a threshold of 0.5.\n",
    "  \n",
    "* Step 7: Evaluate Model Performance\n",
    "  Here is a line describing the metrics:\n",
    "  * Accuracy, Precision, Recall, and F1 Score:** Measure the model's performance by evaluating the proportion of correct predictions, the reliability of positive predictions, the ability to identify actual positive cases, and the balance between precision and recall.\n",
    "  * Confusion Matrix: Evaluates the model's accuracy, sensitivity, and specificity. It compares the predicted labels with the actual labels in the test set.\n",
    "  * ROC and AUC: ROC (Receiver Operating Characteristic) curve and AUC (Area Under the Curve) are used to assess the model's ability to distinguish between churners and non-churners.\n",
    "  * The ROC curve is plotted to visually inspect the model's performance.\n",
    "  \n",
    "* Importance:\n",
    "  * Splitting the data into training and testing sets helps in evaluating the model's generalization.\n",
    "  * Logistic regression is a commonly used method for binary classification tasks like churn prediction.\n",
    "  * Model evaluation through metrics, confusion matrix, ROC, and AUC provides insights into model performance.\n",
    "  * This process is key for building a reliable predictive model to support business decisions.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e00dff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27e00dff",
    "outputId": "d9da8b06-aff7-4737-ebd6-101b46c2c3bf"
   },
   "outputs": [],
   "source": [
    "# Step 3: Split the dataset into training and testing sets\n",
    "# Set random state for reproducibility\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=123, stratify=dataset['Churn'])\n",
    "\n",
    "# Separate features and target variable\n",
    "X_train = train_data.drop('Churn', axis=1)\n",
    "y_train = train_data['Churn']\n",
    "X_test = test_data.drop('Churn', axis=1)\n",
    "y_test = test_data['Churn']\n",
    "\n",
    "# Step 4: Fit the Logistic Regression Model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Model Summary\n",
    "# Since scikit-learn's LogisticRegression does not provide a summary directly, we print coefficients and intercept\n",
    "coefficients = pd.Series(model.coef_[0], index=X_train.columns)\n",
    "intercept = model.intercept_[0]\n",
    "print(\"Coefficients:\")\n",
    "print(coefficients)\n",
    "print(\"Intercept:\", intercept)\n",
    "\n",
    "# Step 6: Make predictions on the test data\n",
    "probabilities = model.predict_proba(X_test)[:, 1]\n",
    "predictions = (probabilities > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b04ac5-e311-4683-9bcb-6a97bb5cf9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold for coefficient significance (you can adjust this)\n",
    "threshold = 0.1\n",
    "\n",
    "# Extract coefficients\n",
    "coefficients = pd.Series(model.coef_[0], index=X_train.columns)\n",
    "\n",
    "# Identify significant coefficients based on the threshold\n",
    "significant_coefficients = coefficients[np.abs(coefficients) > threshold]\n",
    "\n",
    "# Print the significant coefficients\n",
    "print(\"Significant Coefficients:\")\n",
    "print(significant_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rJ6Fl9K3KD40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJ6Fl9K3KD40",
    "outputId": "83b173a5-4724-442e-d603-dc13e18d6d03"
   },
   "outputs": [],
   "source": [
    "# Step 7: Model Performance Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "# Calculate Precision\n",
    "precision = precision_score(y_test, predictions)\n",
    "\n",
    "# Calculate Recall\n",
    "recall = recall_score(y_test, predictions)\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "# Print all the metrics\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W6qt-Sp7MkpJ",
   "metadata": {
    "id": "W6qt-Sp7MkpJ"
   },
   "source": [
    "**How to Interpret:**  \n",
    "- **Accuracy**: High accuracy indicates that the model correctly predicts most of the cases, but it might be misleading if the dataset is imbalanced.\n",
    "- **Precision**: High precision means that when the model predicts a customer will churn, it is usually correct. This is useful when the cost of false positives is high.\n",
    "- **Recall**: High recall means the model successfully identifies most of the actual churners. Important when missing a positive case (false negative) is costly.\n",
    "- **F1 Score**: A higher F1 score indicates a good balance between precision and recall. It's particularly valuable when you need to strike a balance between precision and recall, especially in the context of imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NZJI2Z6GA-4I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NZJI2Z6GA-4I",
    "outputId": "4fe1aad8-4087-4422-93eb-d912f8a38379"
   },
   "outputs": [],
   "source": [
    "# Step 8: Evaluate Model Performance\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(ticks=[0.5, 1.5], labels=['No Churn', 'Churn'])\n",
    "plt.yticks(ticks=[0.5, 1.5], labels=['No Churn', 'Churn'], rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# ROC and AUC\n",
    "roc_auc = roc_auc_score(y_test, probabilities)\n",
    "fpr, tpr, _ = roc_curve(y_test, probabilities)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')  # Diagonal line for reference\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Customer Churn Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PDJ1x7hUM1zU",
   "metadata": {
    "id": "PDJ1x7hUM1zU"
   },
   "source": [
    "**How to Interpret:**  \n",
    "- **Confusion Matrix**: This matrix provides a summary of prediction results by showing the counts of true positives (correctly predicted churn cases), true negatives (correctly predicted non-churn cases), false positives (non-churn predicted as churn), and false negatives (churn predicted as non-churn).  \n",
    "  - **True Positives (TP)** (Bottom-right quadrant): The model correctly predicts churn.  \n",
    "  - **True Negatives (TN)** (Top-left quadrant): The model correctly predicts no churn.  \n",
    "  - **False Positives (FP)** (Top-right quadrant): The model incorrectly predicts churn when there is no churn (Type I error).  \n",
    "  - **False Negatives (FN)** (Bottom-left quadrant): The model incorrectly predicts no churn when there is churn (Type II error).  \n",
    "  - **Interpretation**: A good model will have high values for TP and TN (bottom-right and top-left quadrants) and low values for FP and FN (top-right and bottom-left quadrants).\n",
    "\n",
    "- **ROC Curve and AUC**:  \n",
    "  - **ROC Curve**: This is a graphical representation of the model's ability to distinguish between the positive (churn) and negative (no churn) classes. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. The closer the curve is to the top-left corner, the better the model's performance.\n",
    "  - **AUC (Area Under the Curve)**: This value represents the likelihood that the model ranks a randomly chosen positive instance higher than a randomly chosen negative one.  \n",
    "  - **Interpretation**: An AUC value of 0.5 suggests no discrimination (random guessing), while a value closer to 1 indicates excellent performance. An AUC closer to 1 means the model is better at distinguishing between churners and non-churners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad93241",
   "metadata": {
    "id": "6ad93241"
   },
   "source": [
    "# Impact\n",
    "Given our findings, go back to the mini case brief and answer our key research questions using the above analysis and visualizations.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781a879-086d-4d42-b3a2-96aa8d894f40",
   "metadata": {
    "id": "9781a879-086d-4d42-b3a2-96aa8d894f40"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
